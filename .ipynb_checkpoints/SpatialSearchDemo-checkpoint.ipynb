{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8ea643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import OSGridConverter #To convert from =SGB36 to WGS84\n",
    "import pandas as pd #To use pandas for elegant data handling\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly.express as px\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3db7bb",
   "metadata": {},
   "source": [
    "We read in the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f07eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial corpus contains 138639 documents.\n"
     ]
    }
   ],
   "source": [
    "geograph = pd.read_csv('./data/geograph_mini_corpus.csv', encoding='latin-1')\n",
    "        \n",
    "#sample = geograph.sample(n = 1000) # For testing\n",
    "sample = geograph\n",
    "\n",
    "print(f'The initial corpus contains {len(sample)} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf56fa",
   "metadata": {},
   "source": [
    "Create the three indexes we need:\n",
    "\n",
    "- The spatial index stores documents in square cells and allows us to perform a range query\n",
    "- The postings index indexes terms and the documents in which they are found\n",
    "- The gazetteer indexes toponyms and allows us to look them up\n",
    "\n",
    "The spatial index takes a resolution as a parameter\n",
    "The postings file is restricted to terms from this [paper](https://firstmonday.org/ojs/index.php/fm/article/view/3710/3035) if the argument is true. If false then all terms (including stop words) are indexed.\n",
    "\n",
    "Building these indexes takes time, but only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4317435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use pickles here to load objects that haven't changed. That is much quicker than rebuilding\n",
    "# the indexes from scratch, especially the postings one (where I do NLP on all of our documents)\n",
    "changes = False\n",
    "if changes:\n",
    "    resolution = 10000\n",
    "    limitedTerms = True\n",
    "    si = SpatialIndex(resolution, sample)\n",
    "    postings = Postings(limitedTerms, sample)\n",
    "    gaz = Gazetteer()\n",
    "    f1 = open('si.obj', 'wb')\n",
    "    pickle.dump(si,f1)\n",
    "    f1.close()\n",
    "    f2 = open('postings.obj', 'wb')\n",
    "    pickle.dump(postings,f2)\n",
    "    f2.close()\n",
    "    f3 = open('gaz.obj', 'wb')\n",
    "    pickle.dump(gaz,f3)\n",
    "    f3.close()\n",
    "else:\n",
    "    f1 = open('si.obj', 'rb')\n",
    "    si = pickle.load(f1)\n",
    "    f1.close()\n",
    "    f2 = open('postings.obj', 'rb')\n",
    "    postings = pickle.load(f2)\n",
    "    f2.close()\n",
    "    f3 = open('gaz.obj', 'rb')\n",
    "    gaz = pickle.load(f3)\n",
    "    f3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13806f4d",
   "metadata": {},
   "source": [
    "Demonstrates a look up in the gazetteer. For some toponym types point and bounding box are returned. Bounding box could be used to exclude documents for near queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8e929",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name = 'Ben Nevis'\n",
    "location = gaz.getLocation(name)\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e688c7",
   "metadata": {},
   "source": [
    "Find all documents within a bounding box \"centred\" around the location returned from the gazetteer. Documents are returned with their distances and ranked according to distance from toponym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902beebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDist = 100000\n",
    "docsSpatial = si.rangeQuery(maxDist, (location[0],location[1]))\n",
    "print(f'Found {len(docsSpatial)} documents in a {maxDist}m x {maxDist}m around {name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77ce9d",
   "metadata": {},
   "source": [
    "Do a query and return documents ranked by tf-idf scores in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88612dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'hill beach loch'\n",
    "docsThematic = postings.tfIdf(query)\n",
    "maxThematic = next(iter(docsThematic.items()))[1] # Used to normalise scores\n",
    "print(f'Found {len(docsThematic)} documents using the query: {query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7bf9ca",
   "metadata": {},
   "source": [
    "Find the unique documents from both sets. Typically there will be some duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of unique keys for all the documents retrieved from the thematic and spatial indexes\n",
    "candidates = set(list(docsSpatial.keys()) + list(docsThematic.keys()))\n",
    "\n",
    "print(f'Found {len(candidates)} unique documents merging the thematic and spatial indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283107ce",
   "metadata": {},
   "source": [
    "Rank documents according to spatial (distance) and thematic (tf-idf) scores. 'wspatial' determines the weight of each component. A value of 1.0 means that the ranking is purely spatial, 0.0 considers only tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "wspatial = 0.6 # Weight of the spatial score -> weight of 1 means tfidf is ignored\n",
    "\n",
    "scores = dict()\n",
    "for doc in candidates:\n",
    "    st = 0\n",
    "    ss = 0\n",
    "    if doc in docsThematic:\n",
    "        st = docsThematic[doc]/ maxThematic\n",
    "        #print(f'thematic {st}')\n",
    "    if doc in docsSpatial:\n",
    "        ss = 1- docsSpatial[doc]/ maxDist\n",
    "        #print(f'spatial {docsSpatial[doc]} {ss}')\n",
    "    score = ((1-wspatial) * st) + (wspatial * ss)\n",
    "    scores[doc] = score\n",
    "ranked = dict(sorted(scores.items(), key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0f670",
   "metadata": {},
   "source": [
    "This section of code outputs the results in order. It retrieves the document text from the original data frame to display, not that we didn't use this up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbd305",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "i=0\n",
    "for key in ranked:\n",
    "    score = ranked[key]\n",
    "    row = sample.loc[sample['id'] == key]\n",
    "    title = row.iloc[0]['title']\n",
    "    text = row.iloc[0]['text']\n",
    "    print(f'{score}: {title} - {text}\\n')\n",
    "    if i == n:\n",
    "        break\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c92b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "scores = []\n",
    "label = []\n",
    "i=0\n",
    "for key in ranked:\n",
    "    score = ranked[key]\n",
    "    row = sample.loc[sample['id'] == key]\n",
    "    title = row.iloc[0]['title']\n",
    "    g = OSGridConverter.latlong2grid (row.iloc[0]['lat'], row.iloc[0]['lon'], tag = 'WGS84')\n",
    "    xs.append(g.E)\n",
    "    ys.append(g.N)\n",
    "    label.append(title)\n",
    "    scores.append(score)\n",
    "    if i == n:\n",
    "        break\n",
    "    i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a460f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=xs, y=ys, color=scores, hover_data=[label])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2ffbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialIndex:\n",
    "    \n",
    "    def __init__(self, resolution, sample):\n",
    "        \n",
    "        sample.dropna() # Get rid of problematic rows with nas\n",
    "        \n",
    "        for i in sample.index:\n",
    "            try:\n",
    "                g = OSGridConverter.latlong2grid (sample.at[i, 'lat'], sample.at[i, 'lon'], tag = 'WGS84')\n",
    "                sample.at[i, 'x'] = g.E\n",
    "                sample.at[i, 'y'] = g.N\n",
    "            except ValueError:\n",
    "                #print(\"Problem with a document\", sample.at[i,'id'])\n",
    "                sample = sample.drop(i)\n",
    "\n",
    "        # Now we can set up the parameters for our index        \n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.minx = sample['x'].min()\n",
    "        self.maxx = sample['x'].max()\n",
    "        self.miny = sample['y'].min()\n",
    "        self.maxy = sample['y'].max()\n",
    "\n",
    "        w = self.maxx - self.minx\n",
    "        h = self.maxy - self.miny\n",
    "\n",
    "        nc = int(w/self.resolution) + 1\n",
    "        nr = int(h/self.resolution) + 1\n",
    "\n",
    "        #print(maxx, minx, maxy, miny)\n",
    "        #print(nr, nc)\n",
    "\n",
    "        #Build the spatial index now\n",
    "        self.spatialIndex = pd.DataFrame(index=range(nc),columns=range(nr))\n",
    "\n",
    "        #Now we populate the index with document ids\n",
    "        for index, row in sample.iterrows():\n",
    "            i = int((row['x'] - self.minx)/self.resolution)\n",
    "            j = int((row['y'] - self.miny)/self.resolution)\n",
    "            id = row['id']\n",
    "    \n",
    "            #print(row['id'])\n",
    "            #print(row['x'],row['y'],i,j)\n",
    "            if pd.isnull(self.spatialIndex.at[i,j]):\n",
    "                self.spatialIndex.at[i,j] = {id:(row['x'],row['y'])}\n",
    "            else:\n",
    "                names = self.spatialIndex.at[i,j]\n",
    "                names.update({id:(row['x'],row['y'])})\n",
    "                self.spatialIndex.at[i,j] = names\n",
    "\n",
    "        \n",
    "    def rangeQuery(self, dist, point):\n",
    "        x1 = point[0] - dist/2\n",
    "        x2 = point[0] + dist/2\n",
    "        y1 = point[1] - dist/2\n",
    "        y2 = point[1] + dist/2\n",
    "    \n",
    "        i1 = int((x1 - self.minx)/self.resolution)\n",
    "        j1 = int((y1 - self.miny)/self.resolution)\n",
    "        i2 = int((x2 - self.minx)/self.resolution) + 1\n",
    "        j2 = int((y2 - self.miny)/self.resolution) + 1\n",
    "\n",
    "        # Retrieve only the relevant part of the index\n",
    "        result = self.spatialIndex.iloc[i1:i2, j1:j2]\n",
    "        # Turn the data frame into a 1d list\n",
    "        tlist = result.values.flatten()\n",
    "        # Remove all the nans\n",
    "        filtered = filter(lambda i:not(type(i) is float), tlist)\n",
    "        \n",
    "        #Rank by distance\n",
    "        ranked = {}\n",
    "        for item in filtered:\n",
    "            for key in item:\n",
    "                d = si.dist(point, item[key])\n",
    "                #print(key, item[key], dist)\n",
    "                ranked[key] = d    \n",
    "        ranked = dict(sorted(ranked.items(), key = lambda x: x[1], reverse=False))\n",
    "                \n",
    "        return ranked\n",
    "    \n",
    "    def dist(self, p1, p2):\n",
    "        #print(p1[0], p1[1], p2[0], p2[1])\n",
    "        dist = (((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2)) ** 0.5\n",
    "        #print(dist)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92390576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7fe898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #To use pandas for elegant data handling\n",
    "import spacy #Our NLP tools\n",
    "import math\n",
    "\n",
    "class Postings:\n",
    "    \n",
    "    def __init__(self, firstMondayTerms, sample):\n",
    "        #Load a language model to do NLP\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.ndocs = len(sample)\n",
    "                \n",
    "        # firstMonday works like an inverse stop list, and we only use words in these lists for our posting file\n",
    "        if firstMondayTerms:\n",
    "            list = {}\n",
    "            elements = set(pd.read_csv('./data/elements.txt', header=None)[0])\n",
    "            qualities = set(pd.read_csv('./data/qualities.txt', header=None)[0])\n",
    "            activities = set(pd.read_csv('./data/activities.txt', header=None)[0])\n",
    "\n",
    "            terms = elements.union(qualities).union(activities)\n",
    "            lemmas = ' '.join(str(e) for e in terms)\n",
    "\n",
    "            doc = self.nlp(lemmas)\n",
    "            terms = set()\n",
    "            for token in doc:\n",
    "                terms.add(token.lemma_)\n",
    "                \n",
    "            # Now we process our corpus and create a postings file\n",
    "            docs = self.nlp.pipe(sample.text,n_process=2, batch_size=100)\n",
    "\n",
    "            self.postings = dict()\n",
    "\n",
    "            for (idxRow, s1), (_, s2) in zip(sample.iterrows(), enumerate(docs)):\n",
    "                id = s1.id\n",
    "                for token in s2:\n",
    "                    lemma = token.lemma_\n",
    "                    if lemma in terms:\n",
    "\n",
    "                        if lemma in self.postings:\n",
    "                            tf = self.postings[lemma]\n",
    "                            if id in tf:\n",
    "                                tf[id] = tf[id] + 1\n",
    "                            else:\n",
    "                                tf[id] = 1\n",
    "                        else:\n",
    "                            tf = {id: 1}\n",
    "                        self.postings[lemma] = tf\n",
    "                        \n",
    "    def tfIdf(self, query):\n",
    "        results = {}\n",
    "        qdoc = self.nlp(query)\n",
    "        for token in qdoc:\n",
    "            qt = token.lemma_\n",
    "            if qt in self.postings:\n",
    "                dc = len(self.postings[qt])\n",
    "                idf = math.log10(self.ndocs/(dc + 1))\n",
    "                for doc in self.postings[qt]:\n",
    "                    tf = self.postings[qt][doc]\n",
    "                    tfidf = tf * idf\n",
    "                    if doc in results:\n",
    "                        score = results[doc]\n",
    "                        results[doc] = tfidf + score\n",
    "                    else:\n",
    "                        results[doc] = tfidf\n",
    "        results = dict(sorted(results.items(), key = lambda x: x[1], reverse=True))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622ab43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #To use pandas for elegant data handling\n",
    "# Feature codes in gazetteer are as follows:\n",
    "# A Antiquity (non-Roman)\n",
    "# F Forest or wood\n",
    "# FM Farm\n",
    "# H Hill or mountain\n",
    "# R Antiquity (Roman)\n",
    "# C City\n",
    "# T Town\n",
    "# O Other\n",
    "# W Water feature\n",
    "# X All other features\n",
    "\n",
    "class Gazetteer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gaz = dict()\n",
    "        self.offset = {'C': 2000, 'T':500, 'H':250, 'F':500}\n",
    "        # Read in gazetteer data\n",
    "        os_50k = pd.read_csv('./data/50kgaz2012.txt',sep=':', encoding='utf8', header=None)\n",
    "        os_trimmed = os_50k.drop([0,1,3,4,5,6,7,10,11,12,15,16,17,18,19], axis = 1)\n",
    "        os_trimmed.columns = ['name','y','x','county','type']\n",
    "        for index, row in os_trimmed.iterrows():\n",
    "            name = row['name']\n",
    "            entry = os_trimmed.iloc[index].values \n",
    "            # Store gazetteer in a dictionary of unique names\n",
    "            if name in self.gaz:\n",
    "                entries = self.gaz[name]\n",
    "                entries.append(entry)\n",
    "                self.gaz[name] = entries\n",
    "            else:\n",
    "                self.gaz[name] = [entry]\n",
    "            \n",
    "    def getLocation(self, name):\n",
    "        if (name in self.gaz) == False:\n",
    "            return('Name not found in gazetteer')\n",
    "\n",
    "        if len(self.gaz[name]) > 1:\n",
    "            # We let the user disambiguate\n",
    "            i = 0\n",
    "            print(\"This place name is ambiguous - choose an entry\")\n",
    "            for entry in self.gaz[name]:\n",
    "                print(f'{i}: {name}, {entry[3]}')\n",
    "                i = i + 1\n",
    "            index = int(input(\"Choose a value:\"))\n",
    "            entry = self.gaz[name][index]\n",
    "        else:\n",
    "            entry = self.gaz[name][0]\n",
    "            \n",
    "        print(entry)\n",
    "        x = entry[2]\n",
    "        y = entry[1]\n",
    "            \n",
    "        if entry[4] in self.offset:\n",
    "            diff = self.offset[entry[4]]\n",
    "            return (x,y,x-diff, y-diff, x + diff, y + diff)\n",
    "        else:\n",
    "            return(x,y)\n",
    "                                    \n",
    "    def gazDump(self):\n",
    "        for name in self.gaz:\n",
    "            print(name)\n",
    "            print(self.gaz[name])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84017327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "spacy          : 3.3.1\n",
      "numpy          : 1.21.5\n",
      "pandas         : 1.4.4\n",
      "plotly         : 5.9.0\n",
      "OSGridConverter: 0.1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#output all dependencies so that we can reproduce the notebook (we only need this to set things up for Binder)\n",
    "#%load_ext watermark\n",
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbce06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
