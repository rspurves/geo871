{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a6e9b9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook illustrates how we can use NER to search for placenames in a corpus, and enhance a gazetteer. It uses two datesets to illustrate the concepts.\n",
    "\n",
    "1) [Geograph](https://geograph.org.uk) \n",
    "This site invites users to take pictures in the UK and add descriptions. It has almost 7 million pictures, and the data are licenced using a CC By-SA licence, making them available for research as long as we keep the names of the users, and allow others to have access to any data we might create.\n",
    "\n",
    "2) [Ordnance Survey](https://ordnancesurvey.co.uk/) 50k gazetteer\n",
    "This gazetteer was published under a UK Open Government licence and contains all place name found on 1:50k maps in the UK. It is a legacy product (i.e. not used or updated any more), but it is suitable for our purposes.\n",
    "\n",
    "We are going to look for names found in the Geograph data that don't exist in the gazetteer. Since we know that many names occur multiple times, we will do this locally, to increase the chances that we really find new names.\n",
    "\n",
    "**The first block of our code reads in data and builds a simple spatial index for the gazetteer. We only need to do this once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import OSGridConverter #To convert from =SGB36 to WGS84\n",
    "import pandas as pd #To use pandas for elegant data handling\n",
    "import spacy #Our NLP tools\n",
    "import matplotlib.pyplot as plt #To plot results\n",
    "\n",
    "#Load a language model to do NLP\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we read in the geograph data\n",
    "geograph = pd.read_csv('./data/geograph_mini_corpus.csv', encoding='latin-1')\n",
    "print(len(geograph))\n",
    "geograph.head()\n",
    "\n",
    "sample = geograph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc846e",
   "metadata": {},
   "source": [
    "This block demonstrates the NLP results for a single Geograph document. We use the entities later as potential toponyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300b16f",
   "metadata": {},
   "source": [
    "Here we **draw a sample of documents** from the Geograph data and perform NER on those data. \n",
    "\n",
    "The sample can either be random, or we can define a search term and extract all records containg that term.\n",
    "\n",
    "We can rerun this block to build a new sample. The size of this sample can also be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f97d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a spatial index of our documents\n",
    "\n",
    "#First we need to reproject the data so that we have them in the correct projection\n",
    "for i in sample.index:\n",
    "    try:\n",
    "        g = OSGridConverter.latlong2grid (sample.at[i, 'lat'], sample.at[i, 'lon'], tag = 'WGS84')\n",
    "        sample.at[i, 'x'] = g.E\n",
    "        sample.at[i, 'y'] = g.N\n",
    "    except ValueError:\n",
    "        print(\"Problem with a document\", sample.at[i,'id'])\n",
    "\n",
    "# Now we can set up the parameters for our index        \n",
    "resolution = 10000\n",
    "\n",
    "minx = sample['x'].min()\n",
    "maxx = sample['x'].max()\n",
    "miny = sample['y'].min()\n",
    "maxy = sample['y'].max()\n",
    "\n",
    "w = maxx - minx\n",
    "h = maxy - miny\n",
    "\n",
    "nc = int(w/resolution) + 1\n",
    "nr = int(h/resolution) + 1\n",
    "\n",
    "#print(maxx, minx, maxy, miny)\n",
    "#print(nr, nc)\n",
    "\n",
    "#Build the spatial index now\n",
    "spatialIndex = pd.DataFrame(index=range(nc),columns=range(nr))\n",
    "\n",
    "#Now we populate the index with document ids\n",
    "for index, row in sample.iterrows():\n",
    "    i = int((row['x'] - minx)/resolution)\n",
    "    j = int((row['y'] - miny)/resolution)\n",
    "    id = row['id']\n",
    "    \n",
    "    #print(row['id'])\n",
    "    #print(row['x'],row['y'],i,j)\n",
    "    if pd.isnull(spatialIndex.at[i,j]):\n",
    "        spatialIndex.at[i,j] = {id:(row['x'],row['y'])}\n",
    "    else:\n",
    "        names = spatialIndex.at[i,j]\n",
    "        names.update({id:(row['x'],row['y'])})\n",
    "        spatialIndex.at[i,j] = names\n",
    "\n",
    "spatialIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a range query on the spatial index\n",
    "range = 100000 #(100 km)\n",
    "point = (771500,216500) #Ben Nevis\n",
    "x1 = point[0] - range/2\n",
    "x2 = point[0] + range/2\n",
    "y1 = point[1] - range/2\n",
    "y2 = point[1] + range/2\n",
    "    \n",
    "i1 = int((x1 - minx)/resolution)\n",
    "j1 = int((y1 - miny)/resolution)\n",
    "i2 = int((x2 - minx)/resolution) + 1\n",
    "j2 = int((y2 - miny)/resolution) + 1\n",
    "\n",
    "result = spatialIndex.iloc[j1:j2, i1:i2]\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = {}\n",
    "elements = set(pd.read_csv('./data/elements.txt', header=None)[0])\n",
    "qualities = set(pd.read_csv('./data/qualities.txt', header=None)[0])\n",
    "activities = set(pd.read_csv('./data/activities.txt', header=None)[0])\n",
    "\n",
    "terms = elements.union(qualities).union(activities)\n",
    "lemmas = ' '.join(str(e) for e in terms)\n",
    "\n",
    "\n",
    "doc = nlp(lemmas)\n",
    "terms = set()\n",
    "for token in doc:\n",
    "    terms.add(token.lemma_)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de1b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "sample = geograph.sample(n = m)\n",
    "#sample = geograph\n",
    "\n",
    "# Create a postings list for our geograph documents\n",
    "docs = nlp.pipe(sample.text,n_process=2, batch_size=100)\n",
    "\n",
    "postings = dict()\n",
    "\n",
    "for (idxRow, s1), (_, s2) in zip(sample.iterrows(), enumerate(docs)):\n",
    "    id = s1.id\n",
    "    for token in s2:\n",
    "        lemma = token.lemma_\n",
    "        if lemma in terms:\n",
    "\n",
    "            if lemma in postings:\n",
    "                tf = postings[lemma]\n",
    "                if id in tf:\n",
    "                    tf[id] = tf[id] + 1\n",
    "                else:\n",
    "                    tf[id] = 1\n",
    "            else:\n",
    "                tf = {id: 1}\n",
    "                postings[lemma] = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79585bb5",
   "metadata": {},
   "source": [
    "This block of code does the comparisons. It iterates through all the Geograph documents and does the following:\n",
    "\n",
    "- For each document it returns all the toponyms in the gazetteer cell at that location\n",
    "- Compares each name found by the NER to the list of toponyms in the gazetteer cell, and\n",
    "- Annotates the names as either existing (found in the gazetteer) or new (new names)\n",
    "\n",
    "If we change the sample of names, or the resolution of the gazetteer, then the results of the following **comparison** should change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b24a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are going to compare the gazetteer names with those we found\n",
    "\n",
    "data = list()\n",
    "#We iterate through all our results\n",
    "for dict in results:\n",
    "    #First we get the cell indices for the gazetteer\n",
    "    x = dict.get(\"x\")\n",
    "    y = dict.get(\"y\")\n",
    "    i = int((x - minx)/cellSize)\n",
    "    j = int((y - minx)/cellSize)\n",
    "    try:\n",
    "        #Now we find the names in that cell - n.B. we ignore for now the fact that Geograph cell could be at a boundary\n",
    "        gazNames = gaz.at[i,j]\n",
    "        #Deal with a cell having no values in the gazetteer\n",
    "        if (isinstance(gazNames,set) == False): \n",
    "            #print(type(gazNames))\n",
    "            gazNames = {\"NoNamesFound\"}\n",
    "    except KeyError:\n",
    "        gazNames = {\"NoNamesFound\"}\n",
    "    #Get back the named entities for the text        \n",
    "    ents = dict.get(\"entities\")\n",
    "    #Now we iterate through, and find out if each name is already in the local gazetteer\n",
    "    for ent in ents:        \n",
    "        if (ent.text in gazNames):\n",
    "            data.append([dict.get(\"id\"), \"Existing\", ent.text, ent.label_, x, y]) \n",
    "            #print(\"Found existing name:\", ent.text, ent.label_)\n",
    "        else:\n",
    "            #print(\"Potential new name:\" , ent.text, ent.label_)\n",
    "            data.append([dict.get(\"id\"), \"New\", ent.text, ent.label_, x, y]) \n",
    "#Store the results in a dataframe\n",
    "df = pd.DataFrame(data, columns = ['id', 'status','name','type','x','y'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7380072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split results into existing and candidate names for reporting\n",
    "new = df.loc[df['status'] == 'New']\n",
    "existing = df.loc[df['status'] == 'Existing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db2c71",
   "metadata": {},
   "source": [
    "We output the new names we found for the class 'PERSON'. You could calculate precision here by judging how many of these are really toponyms (since we assume implicitly that 'PERSON' names are actually toponyms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at one example NER class in the candidate names\n",
    "names = new.loc[new['type'] == 'PERSON']\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = len(set(new['name']))\n",
    "ce = len(set(existing['name']))\n",
    "print(\"Found\", ce, \"unique existing names and\", cn, \"unique new candidate names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27380bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output all dependencies so that we can reproduce the notebook (we only need this to set things up for Binder)\n",
    "#%load_ext watermark\n",
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9401db",
   "metadata": {},
   "source": [
    "We plot the locations of the new and existing names as simple scatter plots. A density plot would make more sense here to allow a real comparison, but this gives us a first overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38759e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.scatter(new['x'], new['y'])\n",
    "ax2.scatter(existing['x'], existing['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd1d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
