{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a847cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import OSGridConverter #To convert from =SGB36 to WGS84\n",
    "import pandas as pd #To use pandas for elegant data handling\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1843718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geograph = pd.read_csv('./data/geograph_mini_corpus.csv', encoding='latin-1')\n",
    "        \n",
    "sample = geograph.sample(n = 10000) # For testing\n",
    "\n",
    "si = SpatialIndex(10000, sample)\n",
    "postings = Postings(True, sample)\n",
    "gaz = Gazetteer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483611f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ben Nevis' 771500 216500 'Highland' 'H']\n",
      "(216500, 771500, 216250, 771250, 216750, 771750)\n"
     ]
    }
   ],
   "source": [
    "location = gaz.getLocation('Ben Nevis')\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93369ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1055018: 500794.3406639097,\n",
       " 574101: 502740.85889253125,\n",
       " 682910: 503352.03963131015,\n",
       " 233904: 507179.0618716431,\n",
       " 3174881: 511904.2510284516,\n",
       " 232407: 515467.74234766624,\n",
       " 3087618: 515802.40592789015,\n",
       " 1263239: 522333.4856593056,\n",
       " 235009: 525111.7709602404,\n",
       " 669745: 530595.9291975392,\n",
       " 833091: 555284.9011462494,\n",
       " 833016: 555629.0006479144,\n",
       " 949141: 558172.3765477829,\n",
       " 1329739: 558498.6379580527,\n",
       " 11734: 558804.733337147,\n",
       " 489867: 559513.7133261347,\n",
       " 1484005: 562460.4626993794,\n",
       " 1483519: 568337.2928992079,\n",
       " 3145200: 569120.7880300982,\n",
       " 804905: 571929.4055921587,\n",
       " 2667472: 573317.9257105084,\n",
       " 793564: 576520.5643522181,\n",
       " 3146058: 583429.9365819345,\n",
       " 1217735: 587403.3517949996,\n",
       " 269599: 587637.7285368937,\n",
       " 2872330: 590154.9953029288}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxDist = 100000\n",
    "docsSpatial = si.rangeQuery(maxDist, (location[0],location[1]))\n",
    "docsSpatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3baf302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.447076391653512\n"
     ]
    }
   ],
   "source": [
    "docsThematic = postings.tfIdf('hill mountain summit')\n",
    "maxThematic = next(iter(docsThematic.items()))[1]\n",
    "print(maxThematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eb9489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial 525111.7709602404\n",
      "spatial 515802.40592789015\n",
      "spatial 522333.4856593056\n",
      "spatial 590154.9953029288\n",
      "spatial 559513.7133261347\n",
      "spatial 502740.85889253125\n",
      "spatial 558172.3765477829\n",
      "spatial 503352.03963131015\n",
      "spatial 587637.7285368937\n",
      "spatial 571929.4055921587\n",
      "spatial 500794.3406639097\n",
      "spatial 507179.0618716431\n",
      "spatial 530595.9291975392\n",
      "spatial 555284.9011462494\n",
      "spatial 587403.3517949996\n",
      "spatial 583429.9365819345\n",
      "spatial 558498.6379580527\n",
      "spatial 573317.9257105084\n",
      "spatial 558804.733337147\n",
      "spatial 515467.74234766624\n",
      "spatial 576520.5643522181\n",
      "spatial 511904.2510284516\n",
      "spatial 562460.4626993794\n",
      "spatial 569120.7880300982\n",
      "spatial 555629.0006479144\n",
      "spatial 568337.2928992079\n"
     ]
    }
   ],
   "source": [
    "candidates = set(list(docsSpatial.keys()))\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for doc in candidates:\n",
    "    st = 0\n",
    "    ss = 0\n",
    "    if doc in docsThematic:\n",
    "        st = docsThematic[doc]/ maxThematic\n",
    "        print(f'thematic {st}')\n",
    "    if doc in docsSpatial:\n",
    "        ss = docsSpatial[doc]\n",
    "        print(f'spatial {ss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c7a5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialIndex:\n",
    "    \n",
    "    def __init__(self, resolution, sample):\n",
    "        \n",
    "        sample.dropna() # Get rid of problematic rows with nas\n",
    "        \n",
    "        for i in sample.index:\n",
    "            try:\n",
    "                g = OSGridConverter.latlong2grid (sample.at[i, 'lat'], sample.at[i, 'lon'], tag = 'WGS84')\n",
    "                sample.at[i, 'x'] = g.E\n",
    "                sample.at[i, 'y'] = g.N\n",
    "            except ValueError:\n",
    "                #print(\"Problem with a document\", sample.at[i,'id'])\n",
    "                sample = sample.drop(i)\n",
    "\n",
    "        # Now we can set up the parameters for our index        \n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.minx = sample['x'].min()\n",
    "        self.maxx = sample['x'].max()\n",
    "        self.miny = sample['y'].min()\n",
    "        self.maxy = sample['y'].max()\n",
    "\n",
    "        w = self.maxx - self.minx\n",
    "        h = self.maxy - self.miny\n",
    "\n",
    "        nc = int(w/self.resolution) + 1\n",
    "        nr = int(h/self.resolution) + 1\n",
    "\n",
    "        #print(maxx, minx, maxy, miny)\n",
    "        #print(nr, nc)\n",
    "\n",
    "        #Build the spatial index now\n",
    "        self.spatialIndex = pd.DataFrame(index=range(nc),columns=range(nr))\n",
    "\n",
    "        #Now we populate the index with document ids\n",
    "        for index, row in sample.iterrows():\n",
    "            i = int((row['x'] - self.minx)/self.resolution)\n",
    "            j = int((row['y'] - self.miny)/self.resolution)\n",
    "            id = row['id']\n",
    "    \n",
    "            #print(row['id'])\n",
    "            #print(row['x'],row['y'],i,j)\n",
    "            if pd.isnull(self.spatialIndex.at[i,j]):\n",
    "                self.spatialIndex.at[i,j] = {id:(row['x'],row['y'])}\n",
    "            else:\n",
    "                names = self.spatialIndex.at[i,j]\n",
    "                names.update({id:(row['x'],row['y'])})\n",
    "                self.spatialIndex.at[i,j] = names\n",
    "\n",
    "        \n",
    "    def rangeQuery(self, dist, point):\n",
    "        print(dist)\n",
    "        x1 = point[0] - dist/2\n",
    "        x2 = point[0] + dist/2\n",
    "        y1 = point[1] - dist/2\n",
    "        y2 = point[1] + dist/2\n",
    "    \n",
    "        i1 = int((x1 - self.minx)/self.resolution)\n",
    "        j1 = int((y1 - self.miny)/self.resolution)\n",
    "        i2 = int((x2 - self.minx)/self.resolution) + 1\n",
    "        j2 = int((y2 - self.miny)/self.resolution) + 1\n",
    "\n",
    "        # Retrieve only the relevant part of the index\n",
    "        result = self.spatialIndex.iloc[j1:j2, i1:i2]\n",
    "        # Turn the data frame into a 1d list\n",
    "        tlist = result.values.flatten()\n",
    "        # Remove all the nans\n",
    "        filtered = filter(lambda i:not(type(i) is float), tlist)\n",
    "        \n",
    "        #Rank by distance\n",
    "        ranked = {}\n",
    "        for item in filtered:\n",
    "            for key in item:\n",
    "                d = si.dist(point, item[key])\n",
    "                #print(key, item[key], dist)\n",
    "                ranked[key] = d    \n",
    "        ranked = dict(sorted(ranked.items(), key = lambda x: x[1], reverse=False))\n",
    "                \n",
    "        return ranked\n",
    "    \n",
    "    def dist(self, p1, p2):\n",
    "        #print(p1[0], p1[1], p2[0], p2[1])\n",
    "        dist = (((p1[0] - p2[0]) ** 2) + ((p1[1] - p2[1]) ** 2)) ** 0.5\n",
    "        #print(dist)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e35fce91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si.dist((3,3),(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cab2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb44073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #To use pandas for elegant data handling\n",
    "import spacy #Our NLP tools\n",
    "import math\n",
    "\n",
    "class Postings:\n",
    "    \n",
    "    def __init__(self, firstMondayTerms, sample):\n",
    "        #Load a language model to do NLP\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.ndocs = len(sample)\n",
    "                \n",
    "        # firstMonday works like an inverse stop list, and we only use words in these lists for our posting file\n",
    "        if firstMondayTerms:\n",
    "            list = {}\n",
    "            elements = set(pd.read_csv('./data/elements.txt', header=None)[0])\n",
    "            qualities = set(pd.read_csv('./data/qualities.txt', header=None)[0])\n",
    "            activities = set(pd.read_csv('./data/activities.txt', header=None)[0])\n",
    "\n",
    "            terms = elements.union(qualities).union(activities)\n",
    "            lemmas = ' '.join(str(e) for e in terms)\n",
    "\n",
    "            doc = self.nlp(lemmas)\n",
    "            terms = set()\n",
    "            for token in doc:\n",
    "                terms.add(token.lemma_)\n",
    "                \n",
    "            # Now we process our corpus and create a postings file\n",
    "            docs = self.nlp.pipe(sample.text,n_process=2, batch_size=100)\n",
    "\n",
    "            self.postings = dict()\n",
    "\n",
    "            for (idxRow, s1), (_, s2) in zip(sample.iterrows(), enumerate(docs)):\n",
    "                id = s1.id\n",
    "                for token in s2:\n",
    "                    lemma = token.lemma_\n",
    "                    if lemma in terms:\n",
    "\n",
    "                        if lemma in self.postings:\n",
    "                            tf = self.postings[lemma]\n",
    "                            if id in tf:\n",
    "                                tf[id] = tf[id] + 1\n",
    "                            else:\n",
    "                                tf[id] = 1\n",
    "                        else:\n",
    "                            tf = {id: 1}\n",
    "                        self.postings[lemma] = tf\n",
    "                        \n",
    "    def tfIdf(self, query):\n",
    "        results = {}\n",
    "        qdoc = self.nlp(query)\n",
    "        for token in qdoc:\n",
    "            qt = token.lemma_\n",
    "            if qt in self.postings:\n",
    "                dc = len(self.postings[qt])\n",
    "                idf = math.log10(self.ndocs/(dc + 1))\n",
    "                for doc in self.postings[qt]:\n",
    "                    tf = self.postings[qt][doc]\n",
    "                    tfidf = tf * idf\n",
    "                    if doc in results:\n",
    "                        score = results[doc]\n",
    "                        results[doc] = tfidf + score\n",
    "                    else:\n",
    "                        results[doc] = tfidf\n",
    "        results = dict(sorted(results.items(), key = lambda x: x[1], reverse=True))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1fe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #To use pandas for elegant data handling\n",
    "# Feature codes in gazetteer are as follows:\n",
    "# A Antiquity (non-Roman)\n",
    "# F Forest or wood\n",
    "# FM Farm\n",
    "# H Hill or mountain\n",
    "# R Antiquity (Roman)\n",
    "# C City\n",
    "# T Town\n",
    "# O Other\n",
    "# W Water feature\n",
    "# X All other features\n",
    "\n",
    "class Gazetteer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gaz = dict()\n",
    "        self.offset = {'C': 2000, 'T':500, 'H':250, 'F':500}\n",
    "        # Read in gazetteer data\n",
    "        os_50k = pd.read_csv('./data/50kgaz2012.txt',sep=':', encoding='utf8', header=None)\n",
    "        os_trimmed = os_50k.drop([0,1,3,4,5,6,7,10,11,12,15,16,17,18,19], axis = 1)\n",
    "        os_trimmed.columns = ['name','y','x','county','type']\n",
    "        for index, row in os_trimmed.iterrows():\n",
    "            name = row['name']\n",
    "            entry = os_trimmed.iloc[index].values \n",
    "            # Store gazetteer in a dictionary of unique names\n",
    "            if name in self.gaz:\n",
    "                entries = self.gaz[name]\n",
    "                entries.append(entry)\n",
    "                self.gaz[name] = entries\n",
    "            else:\n",
    "                self.gaz[name] = [entry]\n",
    "            \n",
    "    def getLocation(self, name):\n",
    "        if (name in self.gaz) == False:\n",
    "            return('Name not found in gazetteer')\n",
    "\n",
    "        if len(self.gaz[name]) > 1:\n",
    "            # We let the user disambiguate\n",
    "            i = 0\n",
    "            print(\"This place name is ambigous - choose an entry\")\n",
    "            for entry in self.gaz[name]:\n",
    "                print(f'{i}: {name}, {entry[3]}')\n",
    "                i = i + 1\n",
    "            index = int(input(\"Choose a value:\"))\n",
    "            entry = self.gaz[name][index]\n",
    "        else:\n",
    "            entry = self.gaz[name][0]\n",
    "            \n",
    "        print(entry)\n",
    "        x = entry[2]\n",
    "        y = entry[1]\n",
    "            \n",
    "        if entry[4] in self.offset:\n",
    "            diff = self.offset[entry[4]]\n",
    "            return (x,y,x-diff, y-diff, x + diff, y + diff)\n",
    "        else:\n",
    "            return(x,y)\n",
    "                                    \n",
    "    def gazDump(self):\n",
    "        for name in self.gaz:\n",
    "            print(name)\n",
    "            print(self.gaz[name])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c274844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62c151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
